{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bb7b4-08d2-4151-8629-72758b457db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "# Helper function to play a beep sound\n",
    "def play_simple_beep():\n",
    "    framerate = 44100\n",
    "    duration = 1\n",
    "    t = np.linspace(0, duration, int(framerate * duration))\n",
    "    sound = np.sin(2 * np.pi * 440 * t)\n",
    "    display(Audio(sound, rate=framerate, autoplay=True))\n",
    "\n",
    "# Define the neural network with flexible hidden layers\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers=[128, 64, 32]):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i, hidden_size in enumerate(hidden_layers):\n",
    "            layers.append(nn.Linear(input_size if i == 0 else hidden_layers[i-1], hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "        layers.append(nn.Linear(hidden_layers[-1], 1))  # Output layer\n",
    "        layers.append(nn.Sigmoid())  # Sigmoid activation for binary classification\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# # Data preparation\n",
    "# scaler = StandardScaler()\n",
    "# X_train_np = scaler.fit_transform(X_train)  # Using NumPy arrays directly\n",
    "# X_test_np = scaler.transform(X_test)       # Using NumPy arrays directly\n",
    "\n",
    "# # Convert to PyTorch Tensor\n",
    "# X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "# X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "# y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "config = {\n",
    "    \"input_size\": X_train.shape[1],\n",
    "    \"hidden_layers\": [128, 64,32],\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"epochs\": 500,\n",
    "    \"patience\": 20\n",
    "}\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model, loss function, optimizer, and scheduler\n",
    "model = NeuralNetwork(input_size=config[\"input_size\"], hidden_layers=config[\"hidden_layers\"]).to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training with early stopping\n",
    "start_time = time.time()\n",
    "best_val_loss = np.inf\n",
    "early_stopping_counter = 0\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        predicted_train = (y_pred > 0.5).float()\n",
    "        correct_train += (predicted_train == y_batch).sum().item()\n",
    "        total_train += y_batch.size(0)\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    train_accuracies.append(correct_train / total_train)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_test.to(device))\n",
    "        val_loss = criterion(val_pred, y_test.to(device))\n",
    "        val_losses.append(val_loss.item())\n",
    "        predicted_val = (val_pred > 0.5).float()\n",
    "        val_accuracy = accuracy_score(y_test.cpu(), predicted_val.cpu())\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Reduce LR and track best model\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if early_stopping_counter >= config[\"patience\"]:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test.to(device))\n",
    "    y_test_pred_class = (y_test_pred > 0.5).float()\n",
    "    test_accuracy = accuracy_score(y_test.cpu(), y_test_pred_class.cpu())\n",
    "    print(f\"Test Accuracy: {test_accuracy:.5f}\")\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_test.cpu(), y_test_pred_class.cpu())\n",
    "    ConfusionMatrixDisplay(cm, display_labels=[\"Class 0\", \"Class 1\"]).plot()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Play beep sound and print execution time\n",
    "play_simple_beep()\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
